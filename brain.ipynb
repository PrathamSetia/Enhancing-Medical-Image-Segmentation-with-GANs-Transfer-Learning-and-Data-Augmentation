{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNVLR5dadGudXnJSkd03ES+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrathamSetia/Enhancing-Medical-Image-Segmentation-with-GANs-Transfer-Learning-and-Data-Augmentation/blob/main/brain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/mateuszbuda/lgg-mri-segmentation\")\n",
        "# It will ask for your username and key.\n",
        "# Then you can skip the unzip step because this library unzips it for you!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "G6SeKdHNdHEv",
        "outputId": "c248c111-80f6-422a-bcfd-f5e4879a47cf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from opendatasets) (8.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: Prathamthegreat\n",
            "Your Kaggle Key:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Abort",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAbort\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3214398554.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install opendatasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopendatasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.kaggle.com/datasets/mateuszbuda/lgg-mri-segmentation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# It will ask for your username and key.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Then you can skip the unzip step because this library unzips it for you!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/opendatasets/__init__.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(dataset_id_or_url, data_dir, force, dry_run, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Check for a Kaggle dataset URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_kaggle_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_id_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdownload_kaggle_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_id_or_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdry_run\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdry_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Check for Google Drive URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/opendatasets/utils/kaggle_api.py\u001b[0m in \u001b[0;36mdownload_kaggle_dataset\u001b[0;34m(dataset_url, data_dir, force, dry_run)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'KAGGLE_USERNAME'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your Kaggle username\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'KAGGLE_KEY'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_kaggle_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdry_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/opendatasets/utils/kaggle_api.py\u001b[0m in \u001b[0;36m_get_kaggle_key\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_kaggle_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your Kaggle Key\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhide_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt\u001b[0;34m(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhide_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0mecho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAbort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAbort\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Find all files containing '_mask' (these are the answers)\n",
        "mask_files = glob.glob('/content/lgg-mri-segmentation/lgg-mri-segmentation/kaggle_3m/*/*_mask.tif')\n",
        "\n",
        "data = []\n",
        "\n",
        "for mask_path in mask_files:\n",
        "    # The image path is the same as mask path, but without '_mask'\n",
        "    image_path = mask_path.replace('_mask', '')\n",
        "\n",
        "    # Store in a list\n",
        "    data.append({\"image_path\": image_path, \"mask_path\": mask_path})\n",
        "\n",
        "# Convert to a DataFrame (Table) for easier handling\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(f\"Total MRI Scans found: {len(df)}\")\n",
        "print(df.head()) # Show the first 5 rows"
      ],
      "metadata": {
        "id": "NiMo1oWUaP2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Pick a random row from our dataset\n",
        "random_index = random.randint(0, len(df))\n",
        "row = df.iloc[random_index]\n",
        "\n",
        "# Read the image and the mask\n",
        "image = cv2.imread(row['image_path'])\n",
        "mask = cv2.imread(row['mask_path'])\n",
        "\n",
        "# Plot them\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# MRI Image\n",
        "ax[0].imshow(image)\n",
        "ax[0].set_title(\"Brain MRI Scan\")\n",
        "ax[0].axis(\"off\")\n",
        "\n",
        "# Segmentation Mask\n",
        "ax[1].imshow(mask, cmap='gray')\n",
        "ax[1].set_title(\"Tumor Mask (Ground Truth)\")\n",
        "ax[1].axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WiWBfQviaSBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataframe: 80% Train, 20% Validation\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training Images: {len(train_df)}\")\n",
        "print(f\"Validation Images: {len(val_df)}\")"
      ],
      "metadata": {
        "id": "gpOzGGGOap_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import numpy as np\n",
        "\n",
        "class BrainTumorDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 1. Get the file paths\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        image_path = row['image_path']\n",
        "        mask_path = row['mask_path']\n",
        "\n",
        "        # 2. Read the image and mask using OpenCV\n",
        "        # Images are read in BGR format by default, convert to RGB\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Read mask in grayscale mode (0 for black, 255 for white)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # 3. Apply transformations (Resize, Normalize, etc.)\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        # 4. Preprocess mask: Normalize to 0 and 1, and add a channel dimension\n",
        "        # Current shape is [256, 256], we need [1, 256, 256] for PyTorch\n",
        "        mask = mask / 255.0\n",
        "        mask = mask.unsqueeze(0)\n",
        "\n",
        "        return image, mask.float()\n",
        "\n",
        "# Define the \"Recipe\" (Transforms)\n",
        "# We resize everything to 256x256 pixels\n",
        "transform_recipe = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "print(\"Dataset Class defined!\")"
      ],
      "metadata": {
        "id": "UX6Sbo31ayoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create the Dataset objects\n",
        "train_dataset = BrainTumorDataset(train_df, transform=transform_recipe)\n",
        "val_dataset = BrainTumorDataset(val_df, transform=transform_recipe)\n",
        "\n",
        "# Create the DataLoaders (The \"Waiters\")\n",
        "# batch_size=16 means we feed 16 images at a time to the GPU\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Sanity Check: Let's check the shape of one batch\n",
        "images, masks = next(iter(train_loader))\n",
        "print(f\"Batch Image Shape: {images.shape}\") # Should be [16, 3, 256, 256]\n",
        "print(f\"Batch Mask Shape: {masks.shape}\")   # Should be [16, 1, 256, 256]"
      ],
      "metadata": {
        "id": "Qk7uHcYAa_qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models-pytorch"
      ],
      "metadata": {
        "id": "CziYHOObbOcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# 1. Define the Model\n",
        "model = smp.Unet(\n",
        "    encoder_name=\"resnet34\",        # Use ResNet34 as the \"backbone\"\n",
        "    encoder_weights=\"imagenet\",     # Use pre-trained weights (Transfer Learning)\n",
        "    in_channels=3,                  # Our images have 3 channels (RGB)\n",
        "    classes=1                       # Output is 1 channel (Mask)\n",
        ")\n",
        "\n",
        "# 2. Move model to GPU (Essential for speed)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "print(\"Model loaded and moved to GPU!\")"
      ],
      "metadata": {
        "id": "wJJoORolbRhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function: DiceLoss is standard for segmentation\n",
        "loss_fn = smp.losses.DiceLoss(mode='binary', from_logits=True)\n",
        "\n",
        "# Optimizer: Adam\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "print(\"Training tools are ready.\")"
      ],
      "metadata": {
        "id": "uCJk1XkhcHv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm # This gives us a cool progress bar\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, loss_fn, device):\n",
        "    model.train() # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Progress bar wrapper\n",
        "    loop = tqdm(loader)\n",
        "\n",
        "    for images, masks in loop:\n",
        "        # 1. Move data to GPU\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # 2. Forward Pass (Make a prediction)\n",
        "        predictions = model(images)\n",
        "\n",
        "        # 3. Calculate Loss (How bad was the prediction?)\n",
        "        loss = loss_fn(predictions, masks)\n",
        "\n",
        "        # 4. Backward Pass (Update weights)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update progress bar\n",
        "        running_loss += loss.item()\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "# --- RUN THE TRAINING ---\n",
        "print(\"Starting Training...\")\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    avg_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training Complete!\")"
      ],
      "metadata": {
        "id": "MZD2B1qNcRvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Set model to evaluation mode (turns off training specifics)\n",
        "model.eval()\n",
        "\n",
        "# 2. Get a batch of validation data\n",
        "images, masks = next(iter(val_loader))\n",
        "images = images.to(device)\n",
        "\n",
        "# 3. Predict! (No need to calculate gradients here)\n",
        "with torch.no_grad():\n",
        "    predictions = model(images)\n",
        "    # Convert raw scores (logits) to probabilities (0 to 1)\n",
        "    predictions = torch.sigmoid(predictions)\n",
        "    # Convert probabilities to binary mask (0 or 1)\n",
        "    predictions = (predictions > 0.5).float()\n",
        "\n",
        "# 4. Visualize the first 3 results\n",
        "# Helper function to convert Tensor to Image for plotting\n",
        "def tensor_to_image(tensor):\n",
        "    tensor = tensor.cpu().numpy() # Move to CPU\n",
        "    tensor = tensor.transpose(1, 2, 0) # Move channels to end [H, W, C]\n",
        "    return tensor\n",
        "\n",
        "def tensor_to_mask(tensor):\n",
        "    tensor = tensor.cpu().numpy()\n",
        "    return tensor[0, :, :] # Just take the 2D slice\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(12, 10))\n",
        "\n",
        "for i in range(3):\n",
        "    # Original Image\n",
        "    image = tensor_to_image(images[i])\n",
        "    # Because we normalized the image, we need to un-normalize for pretty display\n",
        "    # (This is a rough un-normalization for visualization)\n",
        "    image = (image * 0.229 + 0.485)\n",
        "\n",
        "    axes[i, 0].imshow(image)\n",
        "    axes[i, 0].set_title(\"MRI Scan\")\n",
        "    axes[i, 0].axis('off')\n",
        "\n",
        "    # Ground Truth\n",
        "    axes[i, 1].imshow(tensor_to_mask(masks[i]), cmap='gray')\n",
        "    axes[i, 1].set_title(\"Doctor's Annotation\")\n",
        "    axes[i, 1].axis('off')\n",
        "\n",
        "    # AI Prediction\n",
        "    axes[i, 2].imshow(tensor_to_mask(predictions[i]), cmap='gray')\n",
        "    axes[i, 2].set_title(\"AI Prediction\")\n",
        "    axes[i, 2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZkdG4-6sda94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# 1. Training Transforms (The \"Hard Mode\")\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "\n",
        "    # Geometric Transforms (Positioning)\n",
        "    A.HorizontalFlip(p=0.5),              # 50% chance to flip L/R\n",
        "    A.ShiftScaleRotate(                   # Shift, Zoom, or Rotate\n",
        "        shift_limit=0.0625,\n",
        "        scale_limit=0.1,\n",
        "        rotate_limit=15,                  # Rotate +/- 15 degrees\n",
        "        p=0.5\n",
        "    ),\n",
        "\n",
        "    # Elastic Transforms (Simulating soft tissue)\n",
        "    # This is crucial for medical data!\n",
        "    A.ElasticTransform(\n",
        "        alpha=1,\n",
        "        sigma=50,\n",
        "        alpha_affine=50,\n",
        "        p=0.2\n",
        "    ),\n",
        "\n",
        "    # Pixel Transforms (Scanner differences)\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "\n",
        "    # Finalize\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# 2. Validation Transforms (The \"Clean Mode\")\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "print(\"Augmentation Pipelines Created!\")"
      ],
      "metadata": {
        "id": "taB3q6qdeOOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Helper to visualize un-normalized images\n",
        "def show_aug(dataset, idx=0):\n",
        "    fig, ax = plt.subplots(1, 5, figsize=(20, 5))\n",
        "\n",
        "    for i in range(5):\n",
        "        # Get image/mask pair from dataset\n",
        "        # Each time we call this, the random augmentation runs again!\n",
        "        image, mask = dataset[idx]\n",
        "\n",
        "        # Un-normalize for display\n",
        "        image = image.cpu().numpy().transpose(1, 2, 0)\n",
        "        image = (image * 0.229 + 0.485)\n",
        "\n",
        "        ax[i].imshow(image)\n",
        "        ax[i].imshow(mask[0], cmap='jet', alpha=0.3) # Overlay mask in color\n",
        "        ax[i].set_title(f\"Augmentation {i+1}\")\n",
        "        ax[i].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Create a temporary dataset just to test the visuals\n",
        "debug_dataset = BrainTumorDataset(train_df, transform=train_transform)\n",
        "\n",
        "# Show the result\n",
        "print(\"Visualizing Augmentations on a single patient...\")\n",
        "show_aug(debug_dataset, idx=10) # Change idx to see different patients"
      ],
      "metadata": {
        "id": "4yIcHZ5UeSnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Update Datasets with new transforms\n",
        "train_dataset = BrainTumorDataset(train_df, transform=train_transform)\n",
        "val_dataset = BrainTumorDataset(val_df, transform=val_transform) # Note: Validation uses clean transform!\n",
        "\n",
        "# 2. Update Loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# 3. Re-initialize Model (Start fresh or keep fine-tuning?)\n",
        "# Let's start fresh to see the pure effect of augmentation\n",
        "model = smp.Unet(encoder_name=\"resnet34\", encoder_weights=\"imagenet\", in_channels=3, classes=1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# 4. Train again (Let's do 5 epochs again)\n",
        "print(\"Starting Robust Training...\")\n",
        "for epoch in range(5):\n",
        "    avg_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
        "    print(f\"Epoch {epoch+1}/5 - Average Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "LhwbWVkUeeel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=100, channels=3, feature_g=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.gen = nn.Sequential(\n",
        "            # Input: N x z_dim x 1 x 1\n",
        "            # Block 1: Create the \"seed\" of the image (4x4 size)\n",
        "            self._block(z_dim, feature_g * 16, 4, 1, 0),  # Output: (feature_g*16) x 4 x 4\n",
        "\n",
        "            # Block 2: Upsample to 8x8\n",
        "            self._block(feature_g * 16, feature_g * 8, 4, 2, 1),\n",
        "\n",
        "            # Block 3: Upsample to 16x16\n",
        "            self._block(feature_g * 8, feature_g * 4, 4, 2, 1),\n",
        "\n",
        "            # Block 4: Upsample to 32x32\n",
        "            self._block(feature_g * 4, feature_g * 2, 4, 2, 1),\n",
        "\n",
        "            # Block 5: Final upsample to 64x64\n",
        "            nn.ConvTranspose2d(\n",
        "                feature_g * 2, channels, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.Tanh() # Output: N x channels x 64 x 64 (Pixel values between -1 and 1)\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels, out_channels, kernel_size, stride, padding, bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gen(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels=3, feature_d=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            # Input: N x channels x 64 x 64\n",
        "            # Block 1: 64x64 -> 32x32\n",
        "            nn.Conv2d(channels, feature_d, 4, 2, 1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            # Block 2: 32x32 -> 16x16\n",
        "            self._block(feature_d, feature_d * 2, 4, 2, 1),\n",
        "\n",
        "            # Block 3: 16x16 -> 8x8\n",
        "            self._block(feature_d * 2, feature_d * 4, 4, 2, 1),\n",
        "\n",
        "            # Block 4: 8x8 -> 4x4\n",
        "            self._block(feature_d * 4, feature_d * 8, 4, 2, 1),\n",
        "\n",
        "            # Output: Single probability (Real vs Fake)\n",
        "            nn.Conv2d(feature_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels, out_channels, kernel_size, stride, padding, bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.disc(x)"
      ],
      "metadata": {
        "id": "NbjaFk1Fgq-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Settings\n",
        "Z_DIM = 100   # Size of the random noise vector\n",
        "CHANNELS = 3  # RGB\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 2. Create the Networks\n",
        "gen = Generator(Z_DIM, CHANNELS).to(device)\n",
        "disc = Discriminator(CHANNELS).to(device)\n",
        "\n",
        "# 3. Initialize weights (Important for GAN stability)\n",
        "def initialize_weights(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "gen.apply(initialize_weights)\n",
        "disc.apply(initialize_weights)\n",
        "\n",
        "# 4. Test with dummy data\n",
        "# Create 8 random noise vectors\n",
        "noise = torch.randn(8, Z_DIM, 1, 1).to(device)\n",
        "fake_images = gen(noise)\n",
        "\n",
        "# Pass fake images to discriminator\n",
        "predictions = disc(fake_images)\n",
        "\n",
        "print(f\"Generator Output Shape: {fake_images.shape}\")\n",
        "# Should be: [8, 3, 64, 64]\n",
        "\n",
        "print(f\"Discriminator Output Shape: {predictions.shape}\")\n",
        "# Should be: [8, 1, 1, 1] (A single probability score for each image)\n"
      ],
      "metadata": {
        "id": "Yt434iRgg3F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Specific Transform for GAN (64x64)\n",
        "gan_transform = A.Compose([\n",
        "    A.Resize(64, 64),\n",
        "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)), # Scale to [-1, 1]\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# 2. Create the GAN Dataset/Loader\n",
        "gan_dataset = BrainTumorDataset(train_df, transform=gan_transform)\n",
        "gan_loader = DataLoader(gan_dataset, batch_size=32, shuffle=True) # Larger batch size helps GANs\n",
        "\n",
        "print(\"GAN Data Loader ready (64x64 images).\")"
      ],
      "metadata": {
        "id": "8WE6UHQahTTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "LR = 0.0002             # Learning Rate (Low is better for stability)\n",
        "BETA1 = 0.5             # Momentum term for Adam\n",
        "EPOCHS = 20             # How long to train (Increase to 100+ for good results)\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "# --- Optimizers ---\n",
        "opt_gen = torch.optim.Adam(gen.parameters(), lr=LR, betas=(BETA1, 0.999))\n",
        "opt_disc = torch.optim.Adam(disc.parameters(), lr=LR, betas=(BETA1, 0.999))\n",
        "\n",
        "# --- Loss Function ---\n",
        "criterion = nn.BCELoss() # Binary Cross Entropy\n",
        "\n",
        "# --- Visualization Fixed Noise ---\n",
        "# We use this to track the SAME 16 random vectors over time\n",
        "fixed_noise = torch.randn(16, Z_DIM, 1, 1).to(device)\n",
        "\n",
        "print(\"Starting GAN Training...\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for batch_idx, (real, _) in enumerate(gan_loader):\n",
        "        real = real.to(device)\n",
        "        batch_size = real.shape[0]\n",
        "\n",
        "        ### 1. TRAIN DISCRIMINATOR: max log(D(x)) + log(1 - D(G(z)))\n",
        "        disc.zero_grad()\n",
        "\n",
        "        # 1a. Train on Real Data\n",
        "        label = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n",
        "        output = disc(real).reshape(-1)\n",
        "        loss_real = criterion(output, label)\n",
        "        loss_real.backward()\n",
        "\n",
        "        # 1b. Train on Fake Data\n",
        "        noise = torch.randn(batch_size, Z_DIM, 1, 1).to(device)\n",
        "        fake = gen(noise)\n",
        "        label.fill_(fake_label)\n",
        "        # .detach() ensures we don't update Generator weights yet\n",
        "        output = disc(fake.detach()).reshape(-1)\n",
        "        loss_fake = criterion(output, label)\n",
        "        loss_fake.backward()\n",
        "\n",
        "        loss_disc = loss_real + loss_fake\n",
        "        opt_disc.step()\n",
        "\n",
        "        ### 2. TRAIN GENERATOR: max log(D(G(z)))\n",
        "        gen.zero_grad()\n",
        "        label.fill_(real_label) # The \"Lie\": We want Disc to think these are real\n",
        "        # We re-run discriminator on the fake images (now with gradients flowing to Gen)\n",
        "        output = disc(fake).reshape(-1)\n",
        "        loss_gen = criterion(output, label)\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "    # --- Visualization Block (End of Epoch) ---\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\")\n",
        "\n",
        "    if (epoch + 1) % 5 == 0: # Every 5 epochs, show us the result\n",
        "        with torch.no_grad():\n",
        "            fake = gen(fixed_noise)\n",
        "            # Make a grid of images\n",
        "            img_grid = torchvision.utils.make_grid(fake, normalize=True)\n",
        "            plt.figure(figsize=(8,8))\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"Generated Images at Epoch {epoch+1}\")\n",
        "            plt.imshow(img_grid.cpu().permute(1, 2, 0)) # Move channels to end\n",
        "            plt.show()"
      ],
      "metadata": {
        "id": "DfGcMO-uhoCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "OUTPUT_DIR = \"/content/synthetic_data\"\n",
        "DEBUG_DIR = \"/content/synthetic_debug_rejects\" # New folder for bad images\n",
        "NUM_IMAGES_TO_GENERATE = 500\n",
        "CONFIDENCE_THRESHOLD = 0.2    # LOWERED: Let's be less strict (was 0.5)\n",
        "MIN_TUMOR_PIXELS = 10         # LOWERED: Accept smaller spots (was 50)\n",
        "MAX_ATTEMPTS = 1000           # Reduced for quick debugging\n",
        "Z_DIM = 100\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(DEBUG_DIR, exist_ok=True)\n",
        "\n",
        "# Set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Define the Normalization transform\n",
        "unet_transform = A.Compose([\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "def generate_data():\n",
        "    # Set models to evaluation mode\n",
        "    gen.eval()\n",
        "    model.eval()\n",
        "\n",
        "    generated_count = 0\n",
        "    attempts = 0\n",
        "    debug_saved = 0\n",
        "\n",
        "    print(f\"üöÄ Starting DEBUG Generation...\")\n",
        "    print(f\"Target: {NUM_IMAGES_TO_GENERATE} images.\")\n",
        "    print(f\"Thresholds: Confidence > {CONFIDENCE_THRESHOLD}, Pixels > {MIN_TUMOR_PIXELS}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while generated_count < NUM_IMAGES_TO_GENERATE:\n",
        "            attempts += 1\n",
        "            if attempts > MAX_ATTEMPTS:\n",
        "                print(f\"\\n‚ö†Ô∏è Max attempts reached. Stopping early.\")\n",
        "                break\n",
        "\n",
        "            # 1. Generate a batch of raw images (64x64) from Noise\n",
        "            noise = torch.randn(16, Z_DIM, 1, 1).to(device)\n",
        "            fake_lowres = gen(noise)\n",
        "\n",
        "            for i in range(fake_lowres.size(0)):\n",
        "                if generated_count >= NUM_IMAGES_TO_GENERATE:\n",
        "                    break\n",
        "\n",
        "                # 2. Post-process Image\n",
        "                img_t = fake_lowres[i].cpu()\n",
        "                img_np = (img_t.permute(1, 2, 0).numpy() * 0.5) + 0.5\n",
        "                img_np = np.clip(img_np, 0, 1)\n",
        "\n",
        "                # 3. Upscale to 256x256\n",
        "                img_highres = cv2.resize(img_np, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                # 4. Prepare for U-Net\n",
        "                aug = unet_transform(image=img_highres.astype(np.float32))\n",
        "                input_tensor = aug['image'].unsqueeze(0).to(device)\n",
        "\n",
        "                # 5. Generate Mask\n",
        "                pred_mask = model(input_tensor)\n",
        "                pred_prob = torch.sigmoid(pred_mask)\n",
        "                mask_binary = (pred_prob > CONFIDENCE_THRESHOLD).float().cpu().numpy()[0, 0]\n",
        "\n",
        "                # 6. Check Result\n",
        "                tumor_size = np.sum(mask_binary)\n",
        "\n",
        "                # Save integers for disk\n",
        "                save_img = (img_highres * 255).astype(np.uint8)\n",
        "                save_img = cv2.cvtColor(save_img, cv2.COLOR_RGB2BGR)\n",
        "                save_mask = (mask_binary * 255).astype(np.uint8)\n",
        "\n",
        "                if tumor_size > MIN_TUMOR_PIXELS:\n",
        "                    # --- SUCCESS ---\n",
        "                    base_name = f\"syn_{generated_count}\"\n",
        "                    cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}.png\"), save_img)\n",
        "                    cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_mask.png\"), save_mask)\n",
        "                    generated_count += 1\n",
        "                    if generated_count % 10 == 0:\n",
        "                        print(f\"Generated {generated_count}...\")\n",
        "\n",
        "                elif debug_saved < 20:\n",
        "                    # --- FAILURE: SAVE FOR INSPECTION ---\n",
        "                    # We save the rejected image so you can see what's wrong\n",
        "                    base_name = f\"REJECTED_{attempts}_{i}\"\n",
        "                    cv2.imwrite(os.path.join(DEBUG_DIR, f\"{base_name}.png\"), save_img)\n",
        "                    cv2.imwrite(os.path.join(DEBUG_DIR, f\"{base_name}_predicted_mask.png\"), save_mask)\n",
        "                    debug_saved += 1\n",
        "\n",
        "    print(f\"\\n‚úÖ Process Complete!\")\n",
        "    print(f\"Total Generated: {generated_count}\")\n",
        "    print(f\"Check '{DEBUG_DIR}' to see why images were rejected.\")\n",
        "\n",
        "generate_data()"
      ],
      "metadata": {
        "id": "vvUAvSC1ip7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def view_rejects():\n",
        "    # Find images in the debug folder\n",
        "    reject_dir = \"/content/synthetic_debug_rejects\"\n",
        "    files = sorted(glob.glob(os.path.join(reject_dir, \"*_predicted_mask.png\")))\n",
        "\n",
        "    if len(files) == 0:\n",
        "        print(\"No rejected images found. Did the generation script run?\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(files)} rejected samples. Showing the first 5...\")\n",
        "\n",
        "    # Show top 5\n",
        "    num_show = min(5, len(files))\n",
        "    fig, axes = plt.subplots(num_show, 2, figsize=(10, 4 * num_show))\n",
        "\n",
        "    if num_show == 1: axes = [axes] # Handle single row case\n",
        "\n",
        "    for i in range(num_show):\n",
        "        # The mask file is \"REJECTED_X_Y_predicted_mask.png\"\n",
        "        mask_path = files[i]\n",
        "        # The image file is \"REJECTED_X_Y.png\"\n",
        "        img_path = mask_path.replace(\"_predicted_mask.png\", \".png\")\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Plot Image\n",
        "        ax_img = axes[i][0] if num_show > 1 else axes[0]\n",
        "        ax_img.imshow(img)\n",
        "        ax_img.set_title(f\"GAN Output {i+1}\")\n",
        "        ax_img.axis('off')\n",
        "\n",
        "        # Plot What U-Net Saw (The Mask)\n",
        "        ax_mask = axes[i][1] if num_show > 1 else axes[1]\n",
        "        ax_mask.imshow(mask, cmap='gray')\n",
        "        ax_mask.set_title(f\"U-Net Prediction (Empty?)\")\n",
        "        ax_mask.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "view_rejects()"
      ],
      "metadata": {
        "id": "Rd-qFx1Gr5fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resume training for 30 more epochs\n",
        "print(\"Resuming GAN Training for 30 more epochs...\")\n",
        "resume_epochs = 30\n",
        "\n",
        "for epoch in range(resume_epochs):\n",
        "    for batch_idx, (real, _) in enumerate(gan_loader):\n",
        "        real = real.to(device)\n",
        "        batch_size = real.shape[0]\n",
        "\n",
        "        # Train Discriminator\n",
        "        disc.zero_grad()\n",
        "        label = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n",
        "        output = disc(real).reshape(-1)\n",
        "        loss_real = criterion(output, label)\n",
        "        loss_real.backward()\n",
        "\n",
        "        noise = torch.randn(batch_size, Z_DIM, 1, 1).to(device)\n",
        "        fake = gen(noise)\n",
        "        label.fill_(fake_label)\n",
        "        output = disc(fake.detach()).reshape(-1)\n",
        "        loss_fake = criterion(output, label)\n",
        "        loss_fake.backward()\n",
        "        opt_disc.step()\n",
        "\n",
        "        # Train Generator\n",
        "        gen.zero_grad()\n",
        "        label.fill_(real_label)\n",
        "        output = disc(fake).reshape(-1)\n",
        "        loss_gen = criterion(output, label)\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "    print(f\"Extra Epoch {epoch+1}/{resume_epochs} Complete\")\n",
        "\n",
        "print(\"Extra Training Done. Try Generating again!\")"
      ],
      "metadata": {
        "id": "CxZJtctzsCpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "def check_system_health():\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model.eval()\n",
        "    gen.eval()\n",
        "\n",
        "    print(\"--- DIAGNOSTIC REPORT ---\")\n",
        "\n",
        "    # TEST 1: IS THE U-NET ALIVE?\n",
        "    # We take a real image from the validation loader\n",
        "    try:\n",
        "        real_images, real_masks = next(iter(val_loader))\n",
        "        real_image = real_images[0].to(device).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(real_image)\n",
        "            pred_prob = torch.sigmoid(pred)\n",
        "            pred_mask = (pred_prob > 0.5).float()\n",
        "\n",
        "        tumor_pixels = pred_mask.sum().item()\n",
        "\n",
        "        print(f\"1. U-Net Health Check:\")\n",
        "        if tumor_pixels == 0:\n",
        "            print(\"   ‚ùå FAILED: U-Net predicted EMPTY mask for a real patient.\")\n",
        "            print(\"   -> DIAGNOSIS: You likely restarted Colab and forgot to run the U-Net training loop (Step 10).\")\n",
        "        else:\n",
        "            print(f\"   ‚úÖ PASSED: U-Net found {tumor_pixels} tumor pixels in a real patient.\")\n",
        "\n",
        "    except NameError:\n",
        "        print(\"   ‚ö†Ô∏è SKIPPED: 'val_loader' not found. Ensure you ran the data loading steps.\")\n",
        "\n",
        "    # TEST 2: IS THE GAN GENERATING SHAPES?\n",
        "    print(\"\\n2. GAN output (Raw):\")\n",
        "    with torch.no_grad():\n",
        "        noise = torch.randn(16, 100, 1, 1).to(device)\n",
        "        fake = gen(noise)\n",
        "\n",
        "        # Visualize\n",
        "        img_grid = torchvision.utils.make_grid(fake, normalize=True, nrow=4)\n",
        "        plt.figure(figsize=(8,8))\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"Raw GAN Output (Do these look like brains?)\")\n",
        "        plt.imshow(img_grid.cpu().permute(1, 2, 0))\n",
        "        plt.show()\n",
        "\n",
        "check_system_health()"
      ],
      "metadata": {
        "id": "D3XErAT7uMNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "OUTPUT_DIR = \"/content/synthetic_data\"\n",
        "NUM_IMAGES_TO_GENERATE = 500\n",
        "CONFIDENCE_THRESHOLD = 0.1    # Very permissive\n",
        "MIN_TUMOR_PIXELS = 10\n",
        "MAX_ATTEMPTS = 5000           # Increased attempts\n",
        "Z_DIM = 100\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "unet_transform = A.Compose([\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "def generate_data_verbose():\n",
        "    gen.eval()\n",
        "    model.eval()\n",
        "\n",
        "    generated_count = 0\n",
        "    attempts = 0\n",
        "\n",
        "    print(f\"üöÄ Starting VERBOSE Generation...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while generated_count < NUM_IMAGES_TO_GENERATE:\n",
        "            attempts += 1\n",
        "            if attempts > MAX_ATTEMPTS:\n",
        "                print(f\"\\n‚ö†Ô∏è Max attempts reached ({MAX_ATTEMPTS}). Stopping.\")\n",
        "                break\n",
        "\n",
        "            # 1. Generate Batch\n",
        "            noise = torch.randn(16, Z_DIM, 1, 1).to(device)\n",
        "            fake_lowres = gen(noise)\n",
        "\n",
        "            for i in range(fake_lowres.size(0)):\n",
        "                if generated_count >= NUM_IMAGES_TO_GENERATE: break\n",
        "\n",
        "                # 2. Process\n",
        "                img_t = fake_lowres[i].cpu()\n",
        "                img_np = (img_t.permute(1, 2, 0).numpy() * 0.5) + 0.5\n",
        "                img_np = np.clip(img_np, 0, 1)\n",
        "                img_highres = cv2.resize(img_np, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                # Rainbow Fix\n",
        "                img_uint8 = (img_highres * 255).astype(np.uint8)\n",
        "                img_gray = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2GRAY)\n",
        "                img_clean = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2RGB)\n",
        "                img_clean_float = img_clean.astype(np.float32) / 255.0\n",
        "\n",
        "                # 3. U-Net Check\n",
        "                aug = unet_transform(image=img_clean_float)\n",
        "                input_tensor = aug['image'].unsqueeze(0).to(device)\n",
        "                pred_mask = model(input_tensor)\n",
        "                pred_prob = torch.sigmoid(pred_mask)\n",
        "                mask_binary = (pred_prob > CONFIDENCE_THRESHOLD).float().cpu().numpy()[0, 0]\n",
        "                tumor_size = np.sum(mask_binary)\n",
        "\n",
        "                # 4. Save or Reject\n",
        "                if tumor_size > MIN_TUMOR_PIXELS:\n",
        "                    base_name = f\"syn_{generated_count}\"\n",
        "                    save_img = cv2.cvtColor(img_clean, cv2.COLOR_RGB2BGR)\n",
        "                    save_mask = (mask_binary * 255).astype(np.uint8)\n",
        "                    cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}.png\"), save_img)\n",
        "                    cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_mask.png\"), save_mask)\n",
        "\n",
        "                    generated_count += 1\n",
        "                    print(f\"‚úÖ Saved Image {generated_count}/{NUM_IMAGES_TO_GENERATE}\")\n",
        "\n",
        "            # Heartbeat Message\n",
        "            if attempts % 50 == 0:\n",
        "                print(f\"   ... Checked {attempts*16} candidate images so far ...\")\n",
        "\n",
        "    print(f\"\\nProcess Complete. Total Saved: {generated_count}\")\n",
        "\n",
        "generate_data_verbose()"
      ],
      "metadata": {
        "id": "sMsLlXujxArJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "OUTPUT_DIR = \"/content/synthetic_data\"\n",
        "# We will temporarily bypass the count to ensure we get debug info\n",
        "NUM_IMAGES_TO_GENERATE = 500\n",
        "CONFIDENCE_THRESHOLD = 0.1\n",
        "MIN_TUMOR_PIXELS = 10\n",
        "MAX_ATTEMPTS = 1000           # Lowered for debug run\n",
        "Z_DIM = 100\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "unet_transform = A.Compose([\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "def generate_data_verbose():\n",
        "    gen.eval()\n",
        "    model.eval()\n",
        "\n",
        "    generated_count = 0\n",
        "    attempts = 0\n",
        "\n",
        "    print(f\"üöÄ Starting DIAGNOSTIC Generation...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while generated_count < NUM_IMAGES_TO_GENERATE:\n",
        "            attempts += 1\n",
        "            if attempts > MAX_ATTEMPTS:\n",
        "                print(f\"\\n‚ö†Ô∏è Max attempts reached. Stopping.\")\n",
        "                break\n",
        "\n",
        "            # 1. Generate Batch\n",
        "            noise = torch.randn(16, Z_DIM, 1, 1).to(device)\n",
        "            fake_lowres = gen(noise)\n",
        "\n",
        "            for i in range(fake_lowres.size(0)):\n",
        "                if generated_count >= NUM_IMAGES_TO_GENERATE: break\n",
        "\n",
        "                # 2. Process\n",
        "                img_t = fake_lowres[i].cpu()\n",
        "                img_np = (img_t.permute(1, 2, 0).numpy() * 0.5) + 0.5\n",
        "                img_np = np.clip(img_np, 0, 1)\n",
        "                img_highres = cv2.resize(img_np, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                # Rainbow Fix\n",
        "                img_uint8 = (img_highres * 255).astype(np.uint8)\n",
        "                img_gray = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2GRAY)\n",
        "                img_clean = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2RGB)\n",
        "                img_clean_float = img_clean.astype(np.float32) / 255.0\n",
        "\n",
        "                # 3. U-Net Check\n",
        "                aug = unet_transform(image=img_clean_float)\n",
        "                input_tensor = aug['image'].unsqueeze(0).to(device)\n",
        "                pred_mask = model(input_tensor)\n",
        "                pred_prob = torch.sigmoid(pred_mask)\n",
        "\n",
        "                # --- DEBUGGING STATS ---\n",
        "                max_conf = pred_prob.max().item()\n",
        "\n",
        "                # Force Save the first 10 images just to see them\n",
        "                if attempts == 1 and i < 10:\n",
        "                    base_name = f\"FORCE_DEBUG_{i}\"\n",
        "                    save_img = cv2.cvtColor(img_clean, cv2.COLOR_RGB2BGR)\n",
        "                    # Multiply mask by 255 to make it visible\n",
        "                    # Use the raw probability map to see faint detections\n",
        "                    save_mask = (pred_prob[0,0].cpu().numpy() * 255).astype(np.uint8)\n",
        "\n",
        "                    cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}.png\"), save_img)\n",
        "                    cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_raw_prob.png\"), save_mask)\n",
        "                    if i == 0: print(f\"üì∏ Saved FORCE_DEBUG examples to {OUTPUT_DIR}\")\n",
        "\n",
        "                mask_binary = (pred_prob > CONFIDENCE_THRESHOLD).float().cpu().numpy()[0, 0]\n",
        "                tumor_size = np.sum(mask_binary)\n",
        "\n",
        "                # 4. Save or Reject\n",
        "                if tumor_size > MIN_TUMOR_PIXELS:\n",
        "                    base_name = f\"syn_{generated_count}\"\n",
        "                    save_img = cv2.cvtColor(img_clean, cv2.COLOR_RGB2BGR)\n",
        "                    save_mask = (mask_binary * 255).astype(np.uint8)\n",
        "                    cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}.png\"), save_img)\n",
        "                    cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_mask.png\"), save_mask)\n",
        "                    generated_count += 1\n",
        "                    print(f\"‚úÖ Saved Image {generated_count}/{NUM_IMAGES_TO_GENERATE}\")\n",
        "\n",
        "            # Heartbeat Message with DEBUG INFO\n",
        "            if attempts % 50 == 0:\n",
        "                print(f\"   ... Checked {attempts*16} images. Last Max Conf: {max_conf:.5f} (Need > {CONFIDENCE_THRESHOLD})\")\n",
        "\n",
        "    print(f\"\\nProcess Complete. Total Saved: {generated_count}\")\n",
        "\n",
        "generate_data_verbose()"
      ],
      "metadata": {
        "id": "FJmVpXozxxCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def inspect_force_debug():\n",
        "    # Look for the specific debug files we saved\n",
        "    debug_dir = \"/content/synthetic_data\"\n",
        "    files = sorted(glob.glob(os.path.join(debug_dir, \"FORCE_DEBUG_*_raw_prob.png\")))\n",
        "\n",
        "    if len(files) == 0:\n",
        "        print(\"‚ùå No FORCE_DEBUG files found.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(files)} debug samples. Showing top 3...\")\n",
        "\n",
        "    for i in range(min(3, len(files))):\n",
        "        mask_path = files[i]\n",
        "        img_path = mask_path.replace(\"_raw_prob.png\", \".png\")\n",
        "\n",
        "        # Load Image (Clean RGB)\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Load Raw Probability (Heatmap of where it thinks tumor MIGHT be)\n",
        "        prob_map = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "        # 1. The Input Image\n",
        "        ax[0].imshow(img)\n",
        "        ax[0].set_title(\"What U-Net Saw (Input)\")\n",
        "        ax[0].axis(\"off\")\n",
        "\n",
        "        # 2. The Raw Probability (Boosted contrast)\n",
        "        # We multiply by 10 to make faint detections visible\n",
        "        ax[1].imshow(prob_map, cmap='jet')\n",
        "        ax[1].set_title(\"Raw Probability Heatmap\")\n",
        "        ax[1].axis(\"off\")\n",
        "\n",
        "        # 3. Histogram (Is the image too dark?)\n",
        "        ax[2].hist(img.ravel(), bins=256, range=[0, 256])\n",
        "        ax[2].set_title(\"Pixel Intensity Distribution\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        # Check stats\n",
        "        print(f\"Sample {i}: Max Probability in heatmap = {prob_map.max()/255:.5f}\")\n",
        "\n",
        "inspect_force_debug()"
      ],
      "metadata": {
        "id": "GCg3Klfny0Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "OUTPUT_DIR = \"/content/synthetic_data\"\n",
        "DEBUG_DIR = \"/content/synthetic_debug_view\"\n",
        "NUM_IMAGES_TO_GENERATE = 500\n",
        "CONFIDENCE_THRESHOLD = 0.1    # Low U-Net threshold\n",
        "MIN_TUMOR_PIXELS = 10\n",
        "MAX_ATTEMPTS = 2000\n",
        "Z_DIM = 100\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(DEBUG_DIR, exist_ok=True)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Standard U-Net normalization\n",
        "unet_transform = A.Compose([\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "def force_dynamic_range(image):\n",
        "    \"\"\"Stretches histogram to full 0-255 range\"\"\"\n",
        "    img_min = image.min()\n",
        "    img_max = image.max()\n",
        "    if img_max - img_min > 1e-5:\n",
        "        stretched = (image - img_min) / (img_max - img_min)\n",
        "        return (stretched * 255).astype(np.uint8)\n",
        "    else:\n",
        "        return (image * 255).astype(np.uint8)\n",
        "\n",
        "def apply_clahe(image):\n",
        "    \"\"\"Boosts contrast locally\"\"\"\n",
        "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    enhanced = clahe.apply(gray)\n",
        "    return cv2.cvtColor(enhanced, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "def heuristic_mask(image):\n",
        "    \"\"\"Fallback: Finds the brightest spot in the image (common for tumors)\"\"\"\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    # Blur to remove noise\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    # Threshold the top 2% brightest pixels\n",
        "    _, mask = cv2.threshold(blurred, 240, 255, cv2.THRESH_BINARY)\n",
        "    return mask / 255.0 # Return as 0.0-1.0\n",
        "\n",
        "def generate_data_final():\n",
        "    gen.eval()\n",
        "    model.eval()\n",
        "\n",
        "    generated_count = 0\n",
        "    attempts = 0\n",
        "\n",
        "    print(f\"üöÄ Starting HYBRID Generation (U-Net + Heuristic Fallback)...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while generated_count < NUM_IMAGES_TO_GENERATE:\n",
        "            attempts += 1\n",
        "            if attempts > MAX_ATTEMPTS:\n",
        "                print(f\"\\n‚ö†Ô∏è Max attempts reached. Stopping.\")\n",
        "                break\n",
        "\n",
        "            # 1. Generate\n",
        "            noise = torch.randn(16, Z_DIM, 1, 1).to(device)\n",
        "            fake_lowres = gen(noise)\n",
        "\n",
        "            for i in range(fake_lowres.size(0)):\n",
        "                if generated_count >= NUM_IMAGES_TO_GENERATE: break\n",
        "\n",
        "                # 2. Process & Contrast Boost\n",
        "                img_t = fake_lowres[i].cpu()\n",
        "                img_np = (img_t.permute(1, 2, 0).numpy() * 0.5) + 0.5\n",
        "                img_np = np.clip(img_np, 0, 1)\n",
        "                img_highres = cv2.resize(img_np, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                # Apply Fixes\n",
        "                img_stretched = force_dynamic_range(img_highres)\n",
        "                img_final = apply_clahe(img_stretched)\n",
        "\n",
        "                # Save Debug View (First 10 only)\n",
        "                if generated_count < 10 and attempts == 1:\n",
        "                    cv2.imwrite(os.path.join(DEBUG_DIR, f\"DEBUG_{i}.png\"), cv2.cvtColor(img_final, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "                # 3. Try U-Net Prediction\n",
        "                img_float = img_final.astype(np.float32) / 255.0\n",
        "                aug = unet_transform(image=img_float)\n",
        "                input_tensor = aug['image'].unsqueeze(0).to(device)\n",
        "                pred_mask = model(input_tensor)\n",
        "                pred_prob = torch.sigmoid(pred_mask)\n",
        "\n",
        "                mask_binary = (pred_prob > CONFIDENCE_THRESHOLD).float().cpu().numpy()[0, 0]\n",
        "\n",
        "                # 4. Logic: U-Net vs Heuristic\n",
        "                method = \"UNet\"\n",
        "                if np.sum(mask_binary) < MIN_TUMOR_PIXELS:\n",
        "                    # U-Net failed, try Heuristic\n",
        "                    mask_binary = heuristic_mask(img_final)\n",
        "                    method = \"Heuristic\"\n",
        "\n",
        "                # 5. Save if EITHER method found something\n",
        "                if np.sum(mask_binary) > MIN_TUMOR_PIXELS:\n",
        "                    base_name = f\"syn_{generated_count}\"\n",
        "                    save_img = cv2.cvtColor(img_final, cv2.COLOR_RGB2BGR)\n",
        "                    save_mask = (mask_binary * 255).astype(np.uint8)\n",
        "\n",
        "                    cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}.png\"), save_img)\n",
        "                    cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_mask.png\"), save_mask)\n",
        "\n",
        "                    generated_count += 1\n",
        "                    if generated_count % 50 == 0:\n",
        "                        print(f\"‚úÖ Saved {generated_count}/{NUM_IMAGES_TO_GENERATE} (Latest: {method})\")\n",
        "\n",
        "            if attempts % 50 == 0:\n",
        "                if generated_count == 0:\n",
        "                     print(f\"   ... Checked {attempts*16} images...\")\n",
        "\n",
        "    print(f\"\\nProcess Complete. Total Saved: {generated_count}\")\n",
        "    print(f\"Images located in: {OUTPUT_DIR}\")\n",
        "\n",
        "generate_data_final()"
      ],
      "metadata": {
        "id": "vlUoi0az03Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import segmentation_models_pytorch as smp\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm  # For progress bar\n",
        "\n",
        "# --- 1. GATHER SYNTHETIC DATA ---\n",
        "# We look for files ending in _mask.png in the synthetic folder\n",
        "syn_mask_paths = glob.glob(\"/content/synthetic_data/*_mask.png\")\n",
        "synthetic_data = []\n",
        "\n",
        "for mask_path in syn_mask_paths:\n",
        "    # reconstruct image path from mask path\n",
        "    image_path = mask_path.replace('_mask.png', '.png')\n",
        "    synthetic_data.append({\"image_path\": image_path, \"mask_path\": mask_path})\n",
        "\n",
        "print(f\"Found {len(synthetic_data)} synthetic examples.\")\n",
        "\n",
        "# --- 2. MERGE WITH REAL DATA ---\n",
        "# Convert to DataFrame\n",
        "syn_df = pd.DataFrame(synthetic_data)\n",
        "\n",
        "# Combine with the original train_df (Make sure train_df exists from Step 4)\n",
        "if len(synthetic_data) > 0:\n",
        "    # We assume 'train_df' exists from your earlier steps.\n",
        "    # If you lost it, you might need to re-run the data splitting step.\n",
        "    combined_train_df = pd.concat([train_df, syn_df], ignore_index=True)\n",
        "    print(f\"Original Real Data: {len(train_df)}\")\n",
        "    print(f\"Synthetic GAN Data: {len(syn_df)}\")\n",
        "    print(f\"TOTAL HYBRID DATASET: {len(combined_train_df)}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No synthetic data found! Training on real data only.\")\n",
        "    combined_train_df = train_df\n",
        "\n",
        "# --- 3. SETUP HYBRID TRAINING ---\n",
        "# We use the 'train_transform' we defined earlier (Augmentation)\n",
        "# This applies rotations/elastic transforms to BOTH real and synthetic data\n",
        "hybrid_dataset = BrainTumorDataset(combined_train_df, transform=train_transform)\n",
        "hybrid_loader = DataLoader(hybrid_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Initialize a BRAND NEW model for the final test\n",
        "# We want to see if training from scratch with more data helps\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "final_model = smp.Unet(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=1\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(final_model.parameters(), lr=0.0001)\n",
        "loss_fn = smp.losses.DiceLoss(mode='binary', from_logits=True)\n",
        "\n",
        "# --- 4. TRAINING LOOP FUNCTION ---\n",
        "def train_one_epoch(model, loader, optimizer, loss_fn, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    loop = tqdm(loader)\n",
        "\n",
        "    for images, masks in loop:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Forward\n",
        "        predictions = model(images)\n",
        "        loss = loss_fn(predictions, masks)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "# --- 5. RUN TRAINING ---\n",
        "EPOCHS = 5 # Increase to 20-50 for best results\n",
        "print(\"\\nüöÄ Starting Hybrid Training (Real + Synthetic)...\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    avg_loss = train_one_epoch(final_model, hybrid_loader, optimizer, loss_fn, device)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Final Model Trained! You can now run the visualization code.\")"
      ],
      "metadata": {
        "id": "rCrb793B1PUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def visualize_predictions(model, loader, device):\n",
        "    # 1. Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # 2. Get a batch of validation data\n",
        "    # We use 'next(iter())' to grab the first batch\n",
        "    images, masks = next(iter(loader))\n",
        "    images = images.to(device)\n",
        "\n",
        "    # 3. Predict!\n",
        "    with torch.no_grad():\n",
        "        predictions = model(images)\n",
        "        # Convert raw scores to probabilities (0 to 1)\n",
        "        predictions = torch.sigmoid(predictions)\n",
        "        # Threshold: If prob > 0.5, it's a tumor\n",
        "        pred_masks = (predictions > 0.5).float()\n",
        "\n",
        "    # 4. Helper to convert tensors for plotting\n",
        "    def tensor_to_image(tensor):\n",
        "        # Move to CPU and numpy\n",
        "        img = tensor.cpu().numpy()\n",
        "        # Change from [Channels, Height, Width] to [Height, Width, Channels]\n",
        "        img = img.transpose(1, 2, 0)\n",
        "        # Undo Normalization roughly for display\n",
        "        img = (img * 0.229 + 0.485)\n",
        "        # Clip to 0-1 range just in case\n",
        "        return np.clip(img, 0, 1)\n",
        "\n",
        "    def tensor_to_mask(tensor):\n",
        "        return tensor.cpu().numpy()[0, :, :]\n",
        "\n",
        "    # 5. Plot the first 3 patients\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
        "\n",
        "    print(f\"--- Results on Unseen Validation Data ---\")\n",
        "\n",
        "    for i in range(3):\n",
        "        # A. Original MRI\n",
        "        ax = axes[i, 0]\n",
        "        ax.imshow(tensor_to_image(images[i]))\n",
        "        ax.set_title(\"MRI Scan (Input)\")\n",
        "        ax.axis('off')\n",
        "\n",
        "        # B. Doctor's Label (Ground Truth)\n",
        "        ax = axes[i, 1]\n",
        "        ax.imshow(tensor_to_mask(masks[i]), cmap='gray')\n",
        "        ax.set_title(\"Doctor's Annotation\")\n",
        "        ax.axis('off')\n",
        "\n",
        "        # C. AI Prediction\n",
        "        ax = axes[i, 2]\n",
        "        ax.imshow(tensor_to_mask(pred_masks[i]), cmap='gray')\n",
        "        ax.set_title(\"Hybrid Model Prediction\")\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Add a green box if prediction is correct, red if wrong (simple check)\n",
        "        # (This is just a visual aid, not a strict metric)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run the function\n",
        "visualize_predictions(final_model, val_loader, device)"
      ],
      "metadata": {
        "id": "ntsC0gpb3CjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def calculate_dice_score(model, loader, device):\n",
        "    model.eval()\n",
        "    dice_scores = []\n",
        "\n",
        "    print(\"--- Starting Evaluation on Validation Set ---\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # 1. Predict\n",
        "            logits = model(images)\n",
        "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "\n",
        "            # 2. Calculate Dice for each image in the batch\n",
        "            # Formula: 2*Intersection / (Area_Pred + Area_Truth)\n",
        "            epsilon = 1e-7 # To prevent division by zero\n",
        "\n",
        "            # Flatten to [Batch_Size, -1] to count pixels easily\n",
        "            preds_flat = preds.view(preds.size(0), -1)\n",
        "            masks_flat = masks.view(masks.size(0), -1)\n",
        "\n",
        "            intersection = (preds_flat * masks_flat).sum(dim=1)\n",
        "            union = preds_flat.sum(dim=1) + masks_flat.sum(dim=1)\n",
        "\n",
        "            dice = (2. * intersection + epsilon) / (union + epsilon)\n",
        "\n",
        "            # Store scores (move to CPU)\n",
        "            dice_scores.extend(dice.cpu().numpy())\n",
        "\n",
        "    # 3. Report\n",
        "    final_score = np.mean(dice_scores)\n",
        "    print(f\"\\n‚úÖ Final Average Dice Score: {final_score:.4f}\")\n",
        "\n",
        "    if final_score > 0.8:\n",
        "        print(\"üåü Result: Excellent! The model is highly accurate.\")\n",
        "    elif final_score > 0.6:\n",
        "        print(\"üëç Result: Good. The model detects tumors well but might miss edges.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Result: Needs Improvement. Try training for more Epochs.\")\n",
        "\n",
        "# Run the calculation\n",
        "# We use 'val_loader' because we want to test on REAL data, not synthetic\n",
        "calculate_dice_score(final_model, val_loader, device)"
      ],
      "metadata": {
        "id": "idV895GZ3f1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Save the model state dictionary\n",
        "save_path = \"brain_tumor_unet_hybrid.pth\"\n",
        "torch.save(final_model.state_dict(), save_path)\n",
        "\n",
        "print(f\"‚úÖ Model saved to {save_path}\")\n",
        "print(f\"   File size: {os.path.getsize(save_path) / 1e6:.2f} MB\")\n",
        "\n",
        "# 2. Download it to your local computer\n",
        "files.download(save_path)"
      ],
      "metadata": {
        "id": "it2rciEu40ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import segmentation_models_pytorch as smp\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. DEFINE THE ARCHITECTURE\n",
        "# You must create an \"empty brain\" that matches exactly what you trained.\n",
        "# If you trained a ResNet34 U-Net, you must create a ResNet34 U-Net here.\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = smp.Unet(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=None, # We don't need ImageNet weights, we are loading our own!\n",
        "    in_channels=3,\n",
        "    classes=1\n",
        ").to(device)\n",
        "\n",
        "# 2. LOAD THE WEIGHTS\n",
        "# This is where we \"open\" the .pth file\n",
        "save_path = \"brain_tumor_unet_hybrid.pth\"\n",
        "\n",
        "# Check if file exists just to be safe\n",
        "try:\n",
        "    model.load_state_dict(torch.load(save_path, map_location=device))\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: Could not find {save_path}. Make sure it's in the same folder.\")\n",
        "\n",
        "# 3. USE IT FOR PREDICTION\n",
        "model.eval() # Set to evaluation mode\n",
        "\n",
        "# Let's pretend we have a new patient image\n",
        "# (Here I'm just creating a dummy black image for demonstration)\n",
        "dummy_image = np.zeros((256, 256, 3), dtype=np.uint8)\n",
        "\n",
        "# Preprocess (Normalize & Convert to Tensor)\n",
        "# Note: In a real app, you'd use the exact same Albumentations transform here\n",
        "img_tensor = torch.from_numpy(dummy_image).permute(2, 0, 1).float().unsqueeze(0).to(device)\n",
        "img_tensor = img_tensor / 255.0 # Simple normalization\n",
        "\n",
        "with torch.no_grad():\n",
        "    prediction = model(img_tensor)\n",
        "    prob_map = torch.sigmoid(prediction)\n",
        "    # If > 50% confidence, it's a tumor\n",
        "    binary_mask = (prob_map > 0.5).float()\n",
        "\n",
        "print(\"Prediction complete. The model is ready to use.\")"
      ],
      "metadata": {
        "id": "v95Z_38j6DgI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}